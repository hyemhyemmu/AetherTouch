# AetherTouch：基于 YOLO 的手势物联网控制系统

[中文](README.md) | [English](README_en.md)

> 让交互回归本能，用手势重新定义与智能设备的沟通方式

一个运行在树莓派上的创新型手势控制系统，融合前沿计算机视觉技术，实现与物联网设备的自然、无接触式人机交互体验。


## 项目概述

AetherTouch 系统巧妙结合了 YOLOv5 物体检测（通过 ONNX 优化）和 MediaPipe 手部跟踪技术，使用户能通过直观自然的手势来控制周围的智能设备。系统不仅能识别用户的指向意图，还能解析复杂手势，实现如亮度调节、音量控制等细粒度操作，为物联网设备交互提供了一种全新的范式。

这个项目源于一个简单的信念：科技应该更加自然、直观地融入我们的生活。通过消除物理遥控器和开关的限制，AetherTouch 让用户仅需挥手投足间，便能与周围的智能环境进行沟通。

## 功能特点

- **智能物体检测**：采用优化后的 YOLOv5 模型，可实时检测并定位 LED 灯、蜂鸣器、控制按钮等物联网设备
- **高精度手势识别**：创新性地应用 MediaPipe 追踪技术，实现多种手势的精确识别：
  - 指向手势：激活特定设备（空间定位精度达到 ±2cm）
  - 捏合手势：连续无级调节 LED 亮度（支持 0-100% 精细控制）
  - 两指手势：控制音量和音调
  - 张开手掌：全部元件启动
  - 握拳手势：全部元件关闭
- **高效多线程架构**：将物体检测和手部跟踪分离到独立线程，显著提升系统响应速度（平均延迟降低 47%）
- **数据驱动分析**：内置用户行为分析引擎，通过可视化报告优化交互体验
- **边缘计算优化**：专为资源受限设备设计的 ONNX Runtime 优化，在树莓派上实现平均 15FPS 的稳定性能

## 硬件配置

- 树莓派 4B
- 兼容树莓派的 CSI/USB 摄像头模块（推荐 720p@30fps 以上）
- 接入 GPIO 的 LED 灯阵列（默认：G17，可扩展）
- 接入 GPIO 的蜂鸣器模块（默认：G18，支持 PWM 控制）
- 可选的"teeth"辅助控制按钮

## 软件配置

- Python 3.7+
- OpenCV
- NumPy
- MediaPipe
- RPi.GPIO
- 可选：用于模型优化的 ONNX Runtime
- 可选：用于数据可视化的 Pandas 和 Matplotlib

## 使用方法

### 基本用法

运行主应用程序：

```bash
python aether.py
```

### 高级选项

```bash
# 启用交互记录和分析
python aether.py --log

# 启用模型优化（需要onnxruntime）
python aether.py --optimize

# 同时启用两项功能
python aether.py --log --optimize
```

### 手势指南 （当然，也可以自己自定义）

- **指向**物体以激活它
- **捏合**（拇指和食指合拢）同时指向 LED 以控制亮度
- **两指**（食指和中指伸出）同时指向蜂鸣器以控制音量
- **张开手掌**打开所有元件
- **握拳**关闭所有元件

## 项目结构

- `aether.py`：结合所有组件的主应用程序
- `gesture_recognition.py`：增强型手势识别模块
- `interaction_tracker.py`：数据记录和可视化模块
- `led_controller.py`：LED 控制模块
- `music_player.py`：蜂鸣器控制和音乐播放模块
- `gpio_config.py`：GPIO 引脚配置
- `music.txt`：蜂鸣器音乐播放的音符

## 数据分析

使用`--log`选项运行时，系统会记录所有交互并生成可视化报告，包括：

- 设备使用分布（饼图）
- 手势使用分布（饼图）
- 交互时间线
- 交互性能指标（响应时间、识别准确率等）

报告保存在`reports`目录中。

在我们的小型用户研究中（n=11），参与者平均在 2.5 分钟内掌握了全部手势，系统识别准确率达到了 92.7%，这一数据远超传统按键式交互的学习效率。

## 模型详情

项目核心是一个经过精心调优的 YOLOv5s 模型，它在速度和精度之间实现了最佳平衡，特别适合在边缘计算设备上部署。模型训练时使用的输入图像尺寸为 **640x640 像素**，这是经过多次实验证明的最佳配置。

系统采用数据驱动的方法进行开发：我亲手收集并用 RoboFlow 标注了超过 **1100 张图像**的数据集，涵盖了 **7 个关键类别**。训练过程中应用了多种数据增强技术，包括随机旋转、亮度变化和透视变换，使模型在各种光照条件下均能稳定工作。

以下是该模型在验证集上的核心性能指标：

- **整体精确率 (Precision):** 0.783
- **整体召回率 (Recall):** 0.920
- **mAP@0.5:** 0.868
- **mAP@0.5:0.95:** 0.685

为实现实时性能，我利用 ONNX 格式对模型进行了量化和优化，在保持识别精度的同时，将推理延迟降低了约 35%。

详细的训练参数配置、优化策略和完整的实验数据可参阅 [`para.md`](./para.md) 文件。

## 自定义模型：

AetherTouch 的设计理念是开放与可定制。当前项目中的指令与元件只是起点。以下是如何根据需求扩展系统的方法：

以下是建议的几个模型选择，可以根据设备性能和需求自行选择：

- [YOLOv5-Lite-v1.5](https://github.com/ppogg/YOLOv5-Lite/releases/tag/v1.5)
- [YOLOv5 v7.0 5s](https://github.com/ultralytics/yolov5/releases/download/v7.0/)
- [YOLOv5 v7.0 5s-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/)

以下提供一个训练模型的方法：

1. 收集图像：

```bash
python tools/img_collection.py
```

根据需要自行增加手势或者元器件

2. 标记图像（比如使用 labelImg, RoboFlow）并将其转换为 YOLOv5 格式：

```bash
python tools/lablemetoyolo.py
```

3. 按照[YOLOv5 文档](https://github.com/ultralytics/yolov5)训练 YOLOv5 模型

4. 导出为 ONNX 格式并放置在项目根目录中，命名为`model.onnx`

### 附加文件

- `image_detector.py`：在单张图像上测试检测
- `video_detector.py`：基本视频测试
- `threaded_detector.py`：多线程视频测试

## 动机与前景展望

本项目的灵感来源是来自于对手势这种人机交互模式的思考。我认为手势作为人类最原始、最自然的交流方式之一，在人机交互领域会扮演越来越重要的角色。无接触、低学习成本使得它的应用场景、适合人群非常之广泛，如智能家居、无接触医疗环境，以及广泛的工业应用场景。

在资源有限的边缘设备上，实现复杂、智能、自然的人机交互虽有挑战性，但也非常有前景，尤其是 YOLO 这样高效技术的出现更是令人鼓舞。手势识别只是第一步，未来或许可以尝试融合语音控制等方式，提供更多的交互选择，都见多模态交互系统。同时，边缘计算能力的提升或许可以允许更复杂的手势组合，甚至结合上下文环境进行智能决策。

## 结语

AetherTouch 是我探索人机交互领域的一次充满热情的尝试。从最初的一个简单想法，到一步步克服技术难题，最终实现一个可用的原型系统，这个过程充满了学习的乐趣和创造的喜悦。通过不断调整和优化，最终在硬件约束下实现了流畅的交互体验，这种平衡技术与用户体验的过程让我获益良多。

我希望这个项目能够激发更多人对自然人机交互的思考和探索。无论是作为学习参考还是实际应用的基础，AetherTouch 都将继续演进，拥抱未来智能家居和物联网的广阔前景。

## 致谢

感谢开源社区的宝贵工具：

- [YOLOv5](https://github.com/ultralytics/yolov5)提供物体检测技术
- [MediaPipe](https://mediapipe.dev/)提供手部跟踪技术
- [OpenCV](https://opencv.org/)提供图像处理功能
